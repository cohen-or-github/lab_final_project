{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IZaD4_-4BE1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ccb623-1bd9-4f3d-a76d-e73332708963",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.10/dist-packages (5.11.1)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.10/dist-packages (from cohere) (1.9.7)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.2)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.9.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence_transformers\n",
        "\n",
        "!pip install datasets\n",
        "!pip install pinecone-client\n",
        "!pip install cohere\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cohere\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset\n",
        "import ast\n",
        "import time\n",
        "from IPython.display import display\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zlb29pNiURZl",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"cohere_api_key.txt\") as f:\n",
        "    COHERE_API_KEY = f.read().strip()\n",
        "with open(\"pinecone_api_key.txt\") as f:\n",
        "    PINECONE_API_KEY = f.read().strip()"
      ],
      "metadata": {
        "id": "wdWwgeANUyFy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Element - Evaluate the optimal source of knowledge and embedding method"
      ],
      "metadata": {
        "id": "1Xe7b4GUMARI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.Client(api_key=COHERE_API_KEY)\n",
        "\n",
        "def format_embedding(embedding_str):\n",
        "    # Remove leading/trailing brackets if they exist, split the string, and convert to floats\n",
        "    embedding = [float(x) for x in embedding_str.replace('[', '').replace(']', '').split()]\n",
        "    # Convert the list to a JSON string\n",
        "    return json.dumps(embedding)\n",
        "\n",
        "\n",
        "\n",
        "def load_preembedded_data(df):\n",
        "\n",
        "    # Parse the embeddings using ast.literal_eval to safely evaluate list-like strings\n",
        "    df['embedding'] = df['embedding'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
        "    df = df.rename(columns={'full_text': 'text'})\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    embeddings = np.stack(df['embedding'].values)\n",
        "\n",
        "    return dataset, embeddings\n",
        "\n",
        "def create_pinecone_index(\n",
        "        index_name: str,\n",
        "        dimension: int,\n",
        "        metric: str = 'cosine',\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a pinecone index if it does not exist\n",
        "    Args:\n",
        "        index_name: The name of the index\n",
        "        dimension: The dimension of the index\n",
        "        metric: The metric to use for the index\n",
        "    Returns:\n",
        "        Pinecone: A pinecone object which can later be used for upserting vectors and connecting to VectorDBs\n",
        "    \"\"\"\n",
        "    from pinecone import Pinecone, ServerlessSpec\n",
        "    print(\"Creating a Pinecone index...\")\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "    if index_name not in existing_indexes:\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=dimension,\n",
        "            # Remember! It is crucial that the metric you will use in your VectorDB will also be a metric your embedding\n",
        "            # model works well with!\n",
        "            metric=metric,\n",
        "            spec=ServerlessSpec(\n",
        "                cloud=\"aws\",\n",
        "                region=\"us-east-1\"\n",
        "            )\n",
        "        )\n",
        "    print(\"Done!\")\n",
        "    return pc\n",
        "\n",
        "def upsert_vectors(\n",
        "        index: \"sexting-nsfw-adultconten-es\",\n",
        "        embeddings: np.ndarray,\n",
        "        dataset: dict,\n",
        "        text_field: str = 'text',\n",
        "        batch_size: int = 128\n",
        "):\n",
        "    \"\"\"\n",
        "    Upsert vectors to a pinecone index\n",
        "    Args:\n",
        "        index: The pinecone index object\n",
        "        embeddings: The embeddings to upsert\n",
        "        dataset: The dataset containing the metadata\n",
        "        batch_size: The batch size to use for upserting\n",
        "    Returns:\n",
        "        An updated pinecone index\n",
        "    \"\"\"\n",
        "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
        "    shape = embeddings.shape\n",
        "\n",
        "    ids = [str(i) for i in range(shape[0])]\n",
        "    meta = [{text_field: text} for text in dataset[text_field]]\n",
        "\n",
        "    # create list of (id, vector, metadata) tuples to be upserted\n",
        "    to_upsert = list(zip(ids, embeddings, meta))\n",
        "\n",
        "    for i in tqdm(range(0, shape[0], batch_size)):\n",
        "        i_end = min(i + batch_size, shape[0])\n",
        "        index.upsert(vectors=to_upsert[i:i_end])\n",
        "    return index\n",
        "\n",
        "def augment_prompt(\n",
        "        query: str,\n",
        "        model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'),\n",
        "        index=None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Augment the prompt with the top 3 results from the knowledge base\n",
        "    Args:\n",
        "        query: The query to augment\n",
        "        index: The vectorstore object\n",
        "    Returns:\n",
        "        str: The augmented prompt\n",
        "    \"\"\"\n",
        "    results = [float(val) for val in list(model.encode(query))]\n",
        "\n",
        "    # get top 3 results from knowledge base\n",
        "    query_results = index.query(\n",
        "        vector=results,\n",
        "        top_k=5,\n",
        "        include_values=True,\n",
        "        include_metadata=True\n",
        "    )['matches']\n",
        "    text_matches = [match['metadata']['text'] for match in query_results]\n",
        "\n",
        "    # get the text from the results\n",
        "    source_knowledge = \"\\n\\n\".join(text_matches)\n",
        "\n",
        "    # feed into an augmented prompt\n",
        "    augmented_prompt = f\"\"\"\n",
        "You are tasked with composing a WhatsApp message that you would send directly to the person, maintaining the tone, style, and level of empathy and directness used in the provided source material.\n",
        "\n",
        "The response should simulate a real-time, casual WhatsApp message.\n",
        "Ensure the tone is empathetic and conversational, while remaining concise and clear.\n",
        "Use the human writing style from the source knowledge as a guide, but note that the source knowledge does not contain direct answers to the query\n",
        "\n",
        "Important Guidelines:\n",
        "The response must directly answer the query as if you are sending the message right now.\n",
        "Maintain the casual tone, while ensuring the message is smooth and empathetic, like a typical WhatsApp conversation.\n",
        "The source knowledge is provided solely to show the desired tone and writing style. It is not to be used as a source of answers or content for the response.\n",
        "Stick strictly to the format of a direct message, avoiding extra advice or unwarranted sympathy.\n",
        "\n",
        "\n",
        "Example Query and Response Format:\n",
        "\n",
        "Query: “I agreed to be a bridesmaid, but now I can’t commit. How can I let the bride know without causing drama?”\n",
        "\n",
        "Response: “Hey, I don’t know if you’ve noticed, but I’m kind of freaking out about this whole bridesmaid thing. I’m so sorry, but I don’t think I can do it anymore. I know how important your wedding is, and I don’t want to let you down, but I’m just not in the right headspace. I hope you understand, and that we can still be cool.”\n",
        "\n",
        "Query: {query}\n",
        "source_knowledge: {source_knowledge}\"\"\"\n",
        "    return augmented_prompt, source_knowledge\n",
        "\n",
        "def get_response_for_queries(df_queries, model, index):\n",
        "  results = []\n",
        "  for index_row, row in df_queries.iterrows():\n",
        "    query = row['query']\n",
        "    augmented_prompt, source_knowledge = augment_prompt(query, model, index)\n",
        "\n",
        "    # Added a delay of 6 seconds between API calls to respect the rate limit\n",
        "    time.sleep(6)\n",
        "\n",
        "    response = co.chat(\n",
        "        model='command-r-plus',\n",
        "        message=augmented_prompt,\n",
        "    )\n",
        "    results.append({'query': query, 'response': response.text})\n",
        "\n",
        "  return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "\n",
        "data_set_lists =['Friends_BERT.csv','Friends_Deberta.csv','Google_Emotions_BERT.csv','Google_Emotions_Deberta.csv','Reddit_Relationship_Advice_BERT.csv','Reddit_Relationship_Advice_Deberta.csv']\n",
        "\n",
        "\n",
        "\n",
        "for data_set in data_set_lists:\n",
        "  df = pd.read_csv(data_set)\n",
        "  df['embedding'] = df['embedding'].apply(format_embedding)\n",
        "  dataset, embeddings = load_preembedded_data(df)\n",
        "  shape = embeddings.shape\n",
        "  print(f\"The embeddings shape: {embeddings.shape}\")\n",
        "  INDEX_NAME_BERT = \"Insert your index name here, ensuring the appropriate dimensions\"\n",
        "  INDEX_NAME_deberta = \"Insert your index name here, ensuring the appropriate dimensions\"\n",
        "  if 'BERT' in data_set:\n",
        "    INDEX_NAME = INDEX_NAME_BERT\n",
        "    EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
        "  elif 'Deberta' in data_set:\n",
        "    INDEX_NAME = INDEX_NAME_deberta\n",
        "    EMBEDDING_MODEL = 'microsoft/deberta-base'\n",
        "  model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "  pc = create_pinecone_index(INDEX_NAME, shape[1])\n",
        "  index = pc.Index(INDEX_NAME)\n",
        "  index_upserted = upsert_vectors(index, embeddings, dataset)\n",
        "  df_queries = pd.read_csv('5_querys.csv')\n",
        "  df_results = get_response_for_queries(df_queries, model, index)\n",
        "  df_results.to_csv(f'_results_{data_set}', index=False)\n"
      ],
      "metadata": {
        "id": "fBfuXWbJhzwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Element - RAG with FRIENDS Source Knowledge\n"
      ],
      "metadata": {
        "id": "MCM5MRfrjeOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Friends_BERT.csv')\n",
        "df['embedding'] = df['embedding'].apply(format_embedding)\n",
        "dataset, embeddings = load_preembedded_data(df)\n",
        "shape = embeddings.shape\n",
        "print(f\"The embeddings shape: {embeddings.shape}\")\n",
        "INDEX_NAME = \"Insert your index name here, ensuring the appropriate dimensions\"\n",
        "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "pc = create_pinecone_index(INDEX_NAME, shape[1])\n",
        "index = pc.Index(INDEX_NAME)\n",
        "index_upserted = upsert_vectors(index, embeddings, dataset)\n",
        "df_queries = pd.read_csv('200_prompts.csv')\n",
        "df_results = get_response_for_queries(df_queries, model, index)\n",
        "df_results.to_csv('df_results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Af5F-HT1H6bd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}